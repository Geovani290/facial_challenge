import torch
import torch.nn as nn
from torch.optim import AdamW, lr_scheduler
from tqdm import tqdm
import numpy as np
import matplotlib.pyplot as plt
from.config import cfg
import os
import sys

# Ajout pour la rÃ©gression ordinale
class OrdinalLoss(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.num_classes = num_classes
        self.criterion = nn.BCEWithLogitsLoss()

    def forward(self, logits, targets):
        targets_ordinal = self.age_to_ordinal_target(targets)
        return self.criterion(logits, targets_ordinal)
    
    def age_to_ordinal_target(self, ages):
        """Convertit les Ã¢ges cibles en cibles ordinales (one-hot cumulatif)"""
        targets = torch.zeros(ages.size(0), self.num_classes).to(cfg.DEVICE)
        for i, age in enumerate(ages):
            # Assure que l'Ã¢ge est un entier et est dans la plage valide
            age_int = min(int(age.item()), self.num_classes)
            if age_int > 0:
                targets[i, :age_int] = 1
        return targets

class AdvancedTrainer:
Â  Â  """Trainer optimisÃ© avec techniques avancÃ©es"""
Â  Â  
Â  Â  def __init__(self, model, train_loader, val_loader):
Â  Â  Â  Â  self.model = model.to(cfg.DEVICE)
Â  Â  Â  Â  self.train_loader = train_loader
Â  Â  Â  Â  self.val_loader = val_loader
Â  Â  Â  Â  
Â  Â  Â  Â  # Loss function
Â  Â  Â  Â  self.criterion_ordinal = OrdinalLoss(num_classes=cfg.MAX_AGE_VALUE)
Â  Â  Â  Â  self.criterion_mae = nn.L1Loss() # Pour la mÃ©trique
Â  Â  Â  Â  
Â  Â  Â  Â  # Tracking
Â  Â  Â  Â  self.train_losses =
Â  Â  Â  Â  self.val_losses =
Â  Â  Â  Â  self.mae_scores =
Â  Â  Â  Â  self.best_mae = float('inf')
        self.patience_counter = 0

Â  Â  def train_epoch(self, freeze_backbone):
Â  Â  Â  Â  """EntraÃ®nement d'une epoch avec gradient clipping"""
Â  Â  Â  Â  self.model.train()
Â  Â  Â  Â  total_loss = 0
Â  Â  Â  Â  progress_bar = tqdm(self.train_loader, desc="EntraÃ®nement")

        # Geler/dÃ©geler le backbone
        for param in self.model.backbone.parameters():
            param.requires_grad = not freeze_backbone

        # Initialiser l'optimiseur et le scheduler pour l'Ã©tape actuelle
        optimizer = AdamW(
            filter(lambda p: p.requires_grad, self.model.parameters()),
            lr=cfg.LEARNING_RATE,
            weight_decay=1e-4
        )
        scheduler = lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=cfg.NUM_EPOCHS
        )

Â  Â  Â  Â  for images, targets in progress_bar:
Â  Â  Â  Â  Â  Â  images, targets = images.to(cfg.DEVICE), targets.to(cfg.DEVICE)
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  optimizer.zero_grad()
Â  Â  Â  Â  Â  Â  logits = self.model(images)
Â  Â  Â  Â  Â  Â  loss = self.criterion_ordinal(logits, targets)
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  loss.backward()
Â  Â  Â  Â  Â  Â  optimizer.step()
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  total_loss += loss.item()
Â  Â  Â  Â  Â  Â  progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'})
Â  Â  Â  Â  
Â  Â  Â  Â  scheduler.step()
Â  Â  Â  Â  return total_loss / len(self.train_loader)
Â  Â  
Â  Â  def validate(self):
Â  Â  Â  Â  """Validation avec mÃ©triques dÃ©taillÃ©es"""
Â  Â  Â  Â  self.model.eval()
Â  Â  Â  Â  total_loss = 0
Â  Â  Â  Â  all_predictions =
Â  Â  Â  Â  all_targets =
Â  Â  Â  Â  
Â  Â  Â  Â  with torch.no_grad():
Â  Â  Â  Â  Â  Â  for images, targets in self.val_loader:
Â  Â  Â  Â  Â  Â  Â  Â  images, targets = images.to(cfg.DEVICE), targets.to(cfg.DEVICE)
Â  Â  Â  Â  Â  Â  Â  Â  logits = self.model(images)
Â  Â  Â  Â  Â  Â  Â  Â  loss = self.criterion_ordinal(logits, targets)
Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  total_loss += loss.item()
                # Calcul de l'Ã¢ge Ã  partir des logits (moyenne des probabilitÃ©s)
                probs = torch.sigmoid(logits)
                predictions = torch.sum(probs > 0.5, dim=1)
Â  Â  Â  Â  Â  Â  Â  Â  all_predictions.extend(predictions.cpu().numpy())
Â  Â  Â  Â  Â  Â  Â  Â  all_targets.extend(targets.cpu().numpy())
Â  Â  Â  Â  
Â  Â  Â  Â  # Calcul MAE
Â  Â  Â  Â  mae = np.mean(np.abs(np.array(all_predictions) - np.array(all_targets)))
Â  Â  Â  Â  return total_loss / len(self.val_loader), mae
Â  Â  
Â  Â  def train(self, epochs=cfg.NUM_EPOCHS):
Â  Â  Â  Â  """Boucle d'entraÃ®nement complÃ¨te"""
Â  Â  Â  Â  print(f"ðŸš€ DÃ©but entraÃ®nement optimisÃ© - {epochs} epochs - Device: {cfg.DEVICE}")

        print("\n--- Ã‰TAPE 1: EntraÃ®nement de la tÃªte de rÃ©gression (Backbone figÃ©) ---")
        for epoch in range(10):  # Ex: 10 Ã©poques pour la premiÃ¨re Ã©tape
            train_loss = self.train_epoch(freeze_backbone=True)
            val_loss, mae = self.validate()
            print(f'Epoch {epoch+1}/10: Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | MAE: {mae:.2f}')
            
        print("\n--- Ã‰TAPE 2: Fine-tuning du modÃ¨le complet (Backbone dÃ©gelÃ©) ---")
        # RÃ©initialise les compteurs pour le suivi
        self.train_losses =
        self.val_losses =
        self.mae_scores =
        self.best_mae = float('inf')
        self.patience_counter = 0

Â  Â  Â  Â  for epoch in range(epochs):
Â  Â  Â  Â  Â  Â  # EntraÃ®nement
Â  Â  Â  Â  Â  Â  train_loss = self.train_epoch(freeze_backbone=False)
Â  Â  Â  Â  Â  Â  self.train_losses.append(train_loss)
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  # Validation
Â  Â  Â  Â  Â  Â  val_loss, mae = self.validate()
Â  Â  Â  Â  Â  Â  self.val_losses.append(val_loss)
Â  Â  Â  Â  Â  Â  self.mae_scores.append(mae)
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  # Sauvegarde meilleur modÃ¨le et early stopping
Â  Â  Â  Â  Â  Â  best_msg = ""
Â  Â  Â  Â  Â  Â  if mae < self.best_mae:
Â  Â  Â  Â  Â  Â  Â  Â  self.best_mae = mae
Â  Â  Â  Â  Â  Â  Â  Â  self.patience_counter = 0 # RÃ©initialise le compteur
Â  Â  Â  Â  Â  Â  Â  Â  self._save_checkpoint(epoch, mae)
Â  Â  Â  Â  Â  Â  Â  Â  best_msg = "â­ NOUVEAU MEILLEUR"
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  self.patience_counter += 1
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  print(f'Epoch {epoch+1}/{epochs}:')
Â  Â  Â  Â  Â  Â  print(f'Â  Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')
Â  Â  Â  Â  Â  Â  print(f'Â  MAE: {mae:.2f} annÃ©es {best_msg}')
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  # Condition d'arrÃªt prÃ©coce basÃ©e sur la patience
Â  Â  Â  Â  Â  Â  if self.patience_counter > cfg.EARLY_STOPPING_PATIENCE:
Â  Â  Â  Â  Â  Â  Â  Â  print(f"ðŸ›‘ ArrÃªt prÃ©coce: MAE non amÃ©liorÃ©e pendant {cfg.EARLY_STOPPING_PATIENCE} Ã©poques.")
Â  Â  Â  Â  Â  Â  Â  Â  break
Â  Â  Â  Â  
Â  Â  Â  Â  self._plot_results()
Â  Â  Â  Â  return self.best_mae
Â  Â  
Â  Â  def _save_checkpoint(self, epoch, mae):
Â  Â  Â  Â  """Sauvegarde du modÃ¨le"""
Â  Â  Â  Â  checkpoint = {
Â  Â  Â  Â  Â  Â  'epoch': epoch,
Â  Â  Â  Â  Â  Â  'model_state_dict': self.model.state_dict(),
Â  Â  Â  Â  Â  Â  'mae': mae,
Â  Â  Â  Â  Â  Â  'train_losses': self.train_losses,
Â  Â  Â  Â  Â  Â  'val_losses': self.val_losses
Â  Â  Â  Â  }
Â  Â  Â  Â  torch.save(checkpoint, cfg.MODEL_SAVE_PATH)
Â  Â  
Â  Â  def _plot_results(self):
Â  Â  Â  Â  """Visualisation des rÃ©sultats"""
Â  Â  Â  Â  plt.figure(figsize=(12, 4))
Â  Â  Â  Â  
Â  Â  Â  Â  plt.subplot(1, 2, 1)
Â  Â  Â  Â  plt.plot(self.train_losses, label='Train Loss')
Â  Â  Â  Â  plt.plot(self.val_losses, label='Val Loss')
Â  Â  Â  Â  plt.legend()
Â  Â  Â  Â  plt.title('Loss')
Â  Â  Â  Â  
Â  Â  Â  Â  plt.subplot(1, 2, 2)
Â  Â  Â  Â  plt.plot(self.mae_scores, label='MAE', color='red')
Â  Â  Â  Â  plt.legend()
Â  Â  Â  Â  plt.title('MAE (annÃ©es)')
Â  Â  Â  Â  
Â  Â  Â  Â  plt.tight_layout()
Â  Â  Â  Â  plt.savefig(cfg.OUTPUT_DIR / 'training_results.png', dpi=300)
Â  Â  Â  Â  plt.show()

def create_advanced_trainer(model, train_loader, val_loader):
Â  Â  return AdvancedTrainer(model, train_loader, val_loader)
